{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8514b973e60779b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:45.053629Z",
     "start_time": "2024-07-23T07:54:44.577465Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:45.066660Z",
     "start_time": "2024-07-23T07:54:44.581878Z"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0a3f807ecf6eece",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:45.079809Z",
     "start_time": "2024-07-23T07:54:44.585331Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "odp_hourly_path=\"../data/odp_met_hourly/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66fb017ba04ce19c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:45.316387Z",
     "start_time": "2024-07-23T07:54:44.589084Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HABP_1H_SYNOP_20240723051604.csv.zip',\n",
       " 'HABP_1H_SYNOP_20240722121605.csv.zip',\n",
       " 'HABP_1H_SYNOP_20240722211603.csv.zip',\n",
       " 'HABP_1H_SYNOP_20240722231606.csv.zip',\n",
       " 'HABP_1H_SYNOP_20240723031604.csv.zip',\n",
       " 'HABP_1H_SYNOP_20240722111604.csv.zip',\n",
       " 'HABP_1H_SYNOP_20240722141605.csv.zip',\n",
       " 'HABP_1H_SYNOP_20240722181605.csv.zip',\n",
       " 'HABP_1H_SYNOP_20240723061604.csv.zip',\n",
       " 'HABP_1H_SYNOP_20240722161605.csv.zip',\n",
       " 'HABP_1H_SYNOP_20240723011604.csv.zip',\n",
       " 'HABP_1H_SYNOP_20240722171603.csv.zip',\n",
       " 'HABP_1H_SYNOP_20240722081608.csv.zip',\n",
       " 'HABP_1H_SYNOP_20240722201603.csv.zip',\n",
       " 'HABP_1H_SYNOP_20240723001603.csv.zip',\n",
       " 'HABP_1H_SYNOP_20240722091606.csv.zip',\n",
       " 'HABP_1H_SYNOP_20240723041604.csv.zip',\n",
       " 'HABP_1H_SYNOP_20240722131605.csv.zip',\n",
       " 'HABP_1H_SYNOP_20240723021605.csv.zip',\n",
       " 'HABP_1H_SYNOP_20240722191604.csv.zip',\n",
       " 'extracted',\n",
       " 'HABP_1H_SYNOP_20240722101604.csv.zip',\n",
       " 'HABP_1H_SYNOP_20240722151605.csv.zip',\n",
       " 'HABP_1H_SYNOP_20240723071605.csv.zip',\n",
       " 'HABP_1H_SYNOP_20240722221604.csv.zip']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(odp_hourly_path)\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c370af8cff95f1fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:45.369323Z",
     "start_time": "2024-07-23T07:54:44.599389Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "fn = files[0]\n",
    "folder=odp_hourly_path+'extracted/'+Path(files[0]).stem\n",
    "\n",
    "if not os.path.exists(folder):\n",
    "    with zipfile.ZipFile(odp_hourly_path+fn, 'r') as zip_ref:\n",
    "        os.mkdir(folder)\n",
    "        zip_ref.extractall(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32ab7ec82dbd0c21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:45.369725Z",
     "start_time": "2024-07-23T07:54:44.604485Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240722191604/HABP_1H_SYNOP_20240722191604.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240722181605/HABP_1H_SYNOP_20240722181605.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240722221604/HABP_1H_SYNOP_20240722221604.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240722201603/HABP_1H_SYNOP_20240722201603.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240722211603/HABP_1H_SYNOP_20240722211603.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240722101604/HABP_1H_SYNOP_20240722101604.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240722131605/HABP_1H_SYNOP_20240722131605.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240722121605/HABP_1H_SYNOP_20240722121605.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240723051604.csv/HABP_1H_SYNOP_20240723051604.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240722111604/HABP_1H_SYNOP_20240722111604.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240722151605/HABP_1H_SYNOP_20240722151605.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240722141605/HABP_1H_SYNOP_20240722141605.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240722171603/HABP_1H_SYNOP_20240722171603.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240722161605/HABP_1H_SYNOP_20240722161605.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240723061604/HABP_1H_SYNOP_20240723061604.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240723071605/HABP_1H_SYNOP_20240723071605.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240723041604/HABP_1H_SYNOP_20240723041604.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240723051604/HABP_1H_SYNOP_20240723051604.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240722091606/HABP_1H_SYNOP_20240722091606.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240723021605/HABP_1H_SYNOP_20240723021605.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240723011604/HABP_1H_SYNOP_20240723011604.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240723001603/HABP_1H_SYNOP_20240723001603.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240722081608/HABP_1H_SYNOP_20240722081608.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240723031604/HABP_1H_SYNOP_20240723031604.csv\n",
      "../data/odp_met_hourly/extracted/HABP_1H_SYNOP_20240722231606/HABP_1H_SYNOP_20240722231606.csv\n"
     ]
    }
   ],
   "source": [
    "test_file = ''\n",
    "\n",
    "def read_file_with_heather(filename):\n",
    "    with open(filename) as fd:\n",
    "        headers = [ next(fd).replace('#','').strip() for i in range(4) ]\n",
    "        print(headers[1])\n",
    "        print(headers[2])\n",
    "        df = pd.read_csv(fd, sep=';', decimal=',')\n",
    "        return headers, df\n",
    "\n",
    "for folder in os.listdir(odp_hourly_path+'extracted/'):\n",
    "    for fn in os.listdir(odp_hourly_path+'extracted/'+folder+\"/\"):\n",
    "        test_file=odp_hourly_path+'extracted/'+folder+'/' + fn\n",
    "        print(test_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71c4a4e6fe4bdbc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:45.410657Z",
     "start_time": "2024-07-23T07:54:44.610687Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15cb8fa3b8120650",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:45.411530Z",
     "start_time": "2024-07-23T07:54:44.615449Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202407222200;        44165;Budapest Újpest                         ; 47.5733;  19.0750;    104.8;  0.0;    ; 25.1;    ; 25.6;    ; 25.1;    ; 25.9;    ;  -999;    ;   -999;    ;  62;    ;    -999;    ;   -999;    ;  -999;     ;  3.0;    ; 318;     ;  9.0;    ; 304;     ;22:11;       ;-999;    ;   -999;    ;  3.3;    ; 318;     ; -999;     ; -999;      ; -999;      ; -999;      ; -999;       ; 24.4;     ; -999;      ;EOR\n",
      "202407222300;        13704;Sopron Kuruc-domb                       ; 47.6783;  16.6022;    232.8;  0.0;    ; 23.1;    ; 23.6;    ; 23.2;    ; 23.9;    ;  -999;    ;   -999;    ;  68;    ;    -999;    ;    0.0;    ;  -999;     ;  5.0;    ; 330;     ; 12.7;    ; 315;     ;23:22;       ;   2;    ;   -999;    ;  5.7;    ; 330;     ; -999;     ; -999;      ; -999;      ; -999;      ; -999;       ; 22.7;     ; -999;      ;EOR\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "headers,df = read_file_with_heather(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "562b56ff43fd52fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:45.536888Z",
     "start_time": "2024-07-23T07:54:44.624216Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>202407222300</th>\n",
       "      <th>14707</th>\n",
       "      <th>Sopronhorpács</th>\n",
       "      <th>47.4806</th>\n",
       "      <th>16.7292</th>\n",
       "      <th>198.7</th>\n",
       "      <th>0.0</th>\n",
       "      <th></th>\n",
       "      <th>23.6</th>\n",
       "      <th>.1</th>\n",
       "      <th>...</th>\n",
       "      <th>.1</th>\n",
       "      <th>-999.3</th>\n",
       "      <th>.2</th>\n",
       "      <th>-999.4</th>\n",
       "      <th>.1</th>\n",
       "      <th>-999.5</th>\n",
       "      <th>.5</th>\n",
       "      <th>-999.6</th>\n",
       "      <th>.3</th>\n",
       "      <th>EOR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202407222300</td>\n",
       "      <td>15310</td>\n",
       "      <td>Szombathely</td>\n",
       "      <td>47.1983</td>\n",
       "      <td>16.6478</td>\n",
       "      <td>200.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>21.4</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>23.8</td>\n",
       "      <td></td>\n",
       "      <td>21.6</td>\n",
       "      <td></td>\n",
       "      <td>18.5</td>\n",
       "      <td></td>\n",
       "      <td>-999</td>\n",
       "      <td></td>\n",
       "      <td>EOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202407222300</td>\n",
       "      <td>15811</td>\n",
       "      <td>Vasvár</td>\n",
       "      <td>47.0511</td>\n",
       "      <td>16.8119</td>\n",
       "      <td>227.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>20.8</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>-999</td>\n",
       "      <td></td>\n",
       "      <td>-999</td>\n",
       "      <td></td>\n",
       "      <td>-999</td>\n",
       "      <td></td>\n",
       "      <td>-999</td>\n",
       "      <td></td>\n",
       "      <td>EOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202407222300</td>\n",
       "      <td>16204</td>\n",
       "      <td>Szentgotthárd Farkasfa</td>\n",
       "      <td>46.9103</td>\n",
       "      <td>16.3094</td>\n",
       "      <td>310.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>19.4</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>21.8</td>\n",
       "      <td></td>\n",
       "      <td>18.4</td>\n",
       "      <td></td>\n",
       "      <td>17.3</td>\n",
       "      <td></td>\n",
       "      <td>-999</td>\n",
       "      <td></td>\n",
       "      <td>EOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202407222300</td>\n",
       "      <td>16414</td>\n",
       "      <td>Nagykutas</td>\n",
       "      <td>46.9258</td>\n",
       "      <td>16.8128</td>\n",
       "      <td>239.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>20.5</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>-999</td>\n",
       "      <td></td>\n",
       "      <td>-999</td>\n",
       "      <td></td>\n",
       "      <td>19.4</td>\n",
       "      <td></td>\n",
       "      <td>-999</td>\n",
       "      <td></td>\n",
       "      <td>EOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>202407222300</td>\n",
       "      <td>17306</td>\n",
       "      <td>Iklódbördőce</td>\n",
       "      <td>46.6056</td>\n",
       "      <td>16.6131</td>\n",
       "      <td>164.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>20.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>-999</td>\n",
       "      <td></td>\n",
       "      <td>-999</td>\n",
       "      <td></td>\n",
       "      <td>19.3</td>\n",
       "      <td></td>\n",
       "      <td>-999</td>\n",
       "      <td></td>\n",
       "      <td>EOR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   202407222300          14707  Sopronhorpács                             \\\n",
       "0  202407222300          15310  Szombathely                                \n",
       "1  202407222300          15811  Vasvár                                     \n",
       "2  202407222300          16204  Szentgotthárd Farkasfa                     \n",
       "3  202407222300          16414  Nagykutas                                  \n",
       "4  202407222300          17306  Iklódbördőce                               \n",
       "\n",
       "    47.4806    16.7292      198.7    0.0         23.6     .1  ...       .1  \\\n",
       "0   47.1983    16.6478      200.1    0.0         21.4         ...            \n",
       "1   47.0511    16.8119      227.0    0.0         20.8         ...            \n",
       "2   46.9103    16.3094      310.7    0.0         19.4         ...            \n",
       "3   46.9258    16.8128      239.1    0.0         20.5         ...            \n",
       "4   46.6056    16.6131      164.3    0.0         20.0         ...            \n",
       "\n",
       "   -999.3       .2  -999.4        .1  -999.5       .5  -999.6       .3  EOR  \n",
       "0    23.8             21.6              18.5             -999           EOR  \n",
       "1    -999             -999              -999             -999           EOR  \n",
       "2    21.8             18.4              17.3             -999           EOR  \n",
       "3    -999             -999              19.4             -999           EOR  \n",
       "4    -999             -999              19.3             -999           EOR  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fee37fc6f3c71ce",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Run from here - utility function definition\n",
    "\n",
    "we are using ```.env``` file and ```get_connection_env()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74dbbe75bb9f462c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:45.537188Z",
     "start_time": "2024-07-23T07:54:44.648410Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from sqlalchemy_utils import drop_database, database_exists, create_database\n",
    "from sqlalchemy import inspect, create_engine\n",
    "local_test_no_auth = True\n",
    "\n",
    "\n",
    "# not used\n",
    "def get_connection_json():\n",
    "    f = open(\"dbconnection.json\")\n",
    "    connection_data = json.load(f)\n",
    "    DB_USER = connection_data[\"DB_USER\"]\n",
    "\n",
    "    DB_PASS = connection_data[\"DB_PASS\"]\n",
    "    DB_HOST = connection_data[\"DB_HOST\"]\n",
    "    DB_PORT = connection_data[\"DB_PORT\"]\n",
    "    DATABASE = connection_data[\"DATABASE\"]\n",
    "    CHARSET = \"utf-8\"\n",
    "    connect_string = 'postgresql+psycopg2://{}:{}@{}:{}/{}'.format(DB_USER, DB_PASS, DB_HOST, DB_PORT, DATABASE)\n",
    "\n",
    "    # for local test with trust auth\n",
    "    if local_test_no_auth:\n",
    "        DB_HOST = 'localhost'\n",
    "        DB_PASS='postgres'\n",
    "        DB_USER='postgres'\n",
    "        connect_string = 'postgresql+psycopg2://{}:{}@{}:{}/{}'.format(DB_USER, DB_PASS, DB_HOST, DB_PORT, DATABASE)\n",
    "\n",
    "\n",
    "    #connect_string = 'postgresql+psycopg2://{}:{}@{}:{}/{}?charset={}'.format(DB_USER, DB_PASS, DB_HOST, DB_PORT, DATABASE,\n",
    "    #                                                                          CHARSET)\n",
    "    '''\n",
    "    if database_exists(connect_string):\n",
    "        drop_database(connect_string)\n",
    "    create_database(connect_string)\n",
    "    '''\n",
    "    if not database_exists(connect_string):\n",
    "        create_database(connect_string)\n",
    "\n",
    "    engine = create_engine(connect_string, connect_args={'client_encoding': CHARSET})\n",
    "\n",
    "\n",
    "    return engine\n",
    "# https://www.geeksforgeeks.org/how-to-create-and-use-env-files-in-python/\n",
    "from dotenv import load_dotenv, dotenv_values \n",
    "\n",
    "load_dotenv()\n",
    "def get_connection_env():\n",
    "\n",
    "    CHARSET = \"utf-8\"\n",
    "  \n",
    "    # for local test with trust auth\n",
    "    DB_HOST = os.getenv(\"DB_HOST\")\n",
    "    DB_USER=os.getenv(\"DB_USER\")\n",
    "    print(\"db usr\",DB_USER)\n",
    "    DB_PORT=os.getenv(\"DB_PORT\")\n",
    "    DB_PASS=os.getenv(\"DB_PASSWORD\") \n",
    "    print(\"db pw:\", DB_PASS)\n",
    "    DATABASE =os.getenv(\"DATABASE_NAME\")\n",
    "    print(\"name:\", DATABASE)\n",
    "\n",
    "    connect_string = 'postgresql+psycopg2://{}:{}@{}:{}/{}'.format(DB_USER, DB_PASS, DB_HOST, DB_PORT, DATABASE)\n",
    "    print(connect_string)\n",
    "\n",
    "    if not database_exists(connect_string):\n",
    "        create_database(connect_string)\n",
    "\n",
    "    engine = create_engine(connect_string, connect_args={'client_encoding': CHARSET})\n",
    "    return engine\n",
    "\n",
    "day_table_name=os.getenv(\"UPDATE_TIMES_TABLE_NAME\") \n",
    "def table_exists(engine,name):\n",
    "    ins = inspect(engine)\n",
    "    ret =ins.dialect.has_table(engine.connect(),name)\n",
    "    print('Table \"{}\" exists: {}'.format(name, ret))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_db_table(tn , engine):\n",
    "    df = None\n",
    "    if table_exists(engine,tn):\n",
    "        df = pd.read_sql_table(day_table_name, engine)\n",
    "    return df\n",
    "def write_table(tn,df,engine):\n",
    "    df.to_sql(tn, engine, if_exists='replace', method='multi', index=False)\n",
    "\n",
    "def add_update_time(lst, engine):\n",
    "    df = pd.DataFrame({'Time':lst})\n",
    "    df.to_sql(day_table_name, engine, if_exists='replace', method='multi', index=False)\n",
    "\n",
    "def append_to_table(name, df, engine):\n",
    "    df.to_sql(name, engine, if_exists='append', method='multi', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c73d0219d7d990",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "check database name set in environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa98f09918be46d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:45.637785Z",
     "start_time": "2024-07-23T07:54:44.657254Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'odp'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv('DATABASE_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b827cb9d7d843b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Check which hours are already loaded in db \n",
    "    - before that bring up docker swarm: ```docker compose up```\n",
    "    - should be run every two days - downloads always only the new data \n",
    "    - the timestamps here are coming from the name of the files, thus they are not whole hours, but can be used to decide whether we need a particular file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb46e1a15e46f893",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.217534Z",
     "start_time": "2024-07-23T07:54:44.661426Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db usr postgres\n",
      "db pw: postgres\n",
      "name: odp\n",
      "postgresql+psycopg2://postgres:postgres@localhost:5432/odp\n",
      "Table \"update_times\" exists: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = get_connection_env()\n",
    "\n",
    "def get_update_times_already_loaded(engine):\n",
    "    lst=[]\n",
    "    if table_exists(engine,day_table_name):\n",
    "        df = pd.read_sql_table(day_table_name, engine)\n",
    "        lst = df['Time'].to_list()\n",
    "    return lst\n",
    "times_already_loaded = get_update_times_already_loaded(engine)\n",
    "times_already_loaded\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13be31ade97ee4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Downloading files  from odp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18ab966f299d5b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.217698Z",
     "start_time": "2024-07-23T07:54:44.877607Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_folder= '../data/odp_met_hourly/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b986f2ed2d435982",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.217807Z",
     "start_time": "2024-07-23T07:54:44.881934Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files_list =os.listdir(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21919f55f2ee8c65",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "get date from filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f4e28c62532d422",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.218245Z",
     "start_time": "2024-07-23T07:54:44.888900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex([], dtype='datetime64[ns]', freq=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.to_datetime(times_already_loaded)\n",
    "#get_date(files_list[0])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa5b4d9b1140ac52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.218744Z",
     "start_time": "2024-07-23T07:54:44.899681Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting date from HABP_1H_SYNOP_20240721111603.csv.zip ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timestamp('2024-07-21 11:16:03')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn= \"HABP_1H_SYNOP_20240721111603.csv.zip\"   \n",
    "# helper method for parsing filename to date and time\n",
    "def get_datetime(fn):\n",
    "    try:\n",
    "         print(\"getting date from {} ...\".format(fn))\n",
    "             \n",
    "         srt = fn.split('.')[0].split('_')[-1]\n",
    "\n",
    "         dt = pd.to_datetime(srt,format='%Y%m%d%H%M%S')\n",
    "         '''year=int(fn[8:12])\n",
    "         month=int(fn[12:14])\n",
    "         day=int(fn[14:16])\n",
    "         print(year, month,day)\n",
    "         d= date(year,month,day)\n",
    "         print(type(d),d)'''\n",
    "         return dt\n",
    "    except Exception as e:\n",
    "        print('Exception:',e)\n",
    "        return None\n",
    "get_datetime(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62912929f99221c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "download files from omsz and put content in dataframe - then push to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d1265e99b80d6fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.250223Z",
     "start_time": "2024-07-23T07:54:44.915272Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table \"update_times\" exists: False\n",
      "Not searched file https://odp.met.hu\n",
      "Not searched file ?C=N;O=D\n",
      "Not searched file ?C=M;O=A\n",
      "Not searched file ?C=S;O=A\n",
      "Not searched file ?C=D;O=A\n",
      "Not searched file /weather/weather_reports/synoptic/hungary/hourly/\n",
      "Not searched file /weather/weather_reports/synoptic/hungary/hourly/\n",
      "HABP_1H_SYNOP_20240722081608.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722081608.csv.zip ...\n",
      "File found for 2024-07-22 08:16:08\n",
      "Downloading HABP_1H_SYNOP_20240722081608.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722081608.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-21 19:00:00\n",
      "1   2024-07-22 06:00:00\n",
      "2   2024-07-22 06:00:00\n",
      "3   2024-07-22 07:00:00\n",
      "4   2024-07-22 07:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722081608.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722081608.csv.zip ...\n",
      "File found for 2024-07-22 08:16:08\n",
      "Downloading HABP_1H_SYNOP_20240722081608.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722081608.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-21 19:00:00\n",
      "1   2024-07-22 06:00:00\n",
      "2   2024-07-22 06:00:00\n",
      "3   2024-07-22 07:00:00\n",
      "4   2024-07-22 07:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722091606.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722091606.csv.zip ...\n",
      "File found for 2024-07-22 09:16:06\n",
      "Downloading HABP_1H_SYNOP_20240722091606.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722091606.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 08:00:00\n",
      "1   2024-07-22 09:00:00\n",
      "2   2024-07-22 09:00:00\n",
      "3   2024-07-22 09:00:00\n",
      "4   2024-07-22 09:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722091606.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722091606.csv.zip ...\n",
      "File found for 2024-07-22 09:16:06\n",
      "Downloading HABP_1H_SYNOP_20240722091606.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722091606.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 08:00:00\n",
      "1   2024-07-22 09:00:00\n",
      "2   2024-07-22 09:00:00\n",
      "3   2024-07-22 09:00:00\n",
      "4   2024-07-22 09:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722101604.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722101604.csv.zip ...\n",
      "File found for 2024-07-22 10:16:04\n",
      "Downloading HABP_1H_SYNOP_20240722101604.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722101604.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 09:00:00\n",
      "1   2024-07-22 10:00:00\n",
      "2   2024-07-22 10:00:00\n",
      "3   2024-07-22 10:00:00\n",
      "4   2024-07-22 10:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722101604.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722101604.csv.zip ...\n",
      "File found for 2024-07-22 10:16:04\n",
      "Downloading HABP_1H_SYNOP_20240722101604.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722101604.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 09:00:00\n",
      "1   2024-07-22 10:00:00\n",
      "2   2024-07-22 10:00:00\n",
      "3   2024-07-22 10:00:00\n",
      "4   2024-07-22 10:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722111604.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722111604.csv.zip ...\n",
      "File found for 2024-07-22 11:16:04\n",
      "Downloading HABP_1H_SYNOP_20240722111604.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722111604.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 00:00:00\n",
      "1   2024-07-22 03:00:00\n",
      "2   2024-07-22 03:00:00\n",
      "3   2024-07-22 07:00:00\n",
      "4   2024-07-22 08:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722111604.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722111604.csv.zip ...\n",
      "File found for 2024-07-22 11:16:04\n",
      "Downloading HABP_1H_SYNOP_20240722111604.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722111604.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 00:00:00\n",
      "1   2024-07-22 03:00:00\n",
      "2   2024-07-22 03:00:00\n",
      "3   2024-07-22 07:00:00\n",
      "4   2024-07-22 08:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722121605.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722121605.csv.zip ...\n",
      "File found for 2024-07-22 12:16:05\n",
      "Downloading HABP_1H_SYNOP_20240722121605.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722121605.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 01:00:00\n",
      "1   2024-07-22 11:00:00\n",
      "2   2024-07-22 11:00:00\n",
      "3   2024-07-22 12:00:00\n",
      "4   2024-07-22 12:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722121605.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722121605.csv.zip ...\n",
      "File found for 2024-07-22 12:16:05\n",
      "Downloading HABP_1H_SYNOP_20240722121605.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722121605.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 01:00:00\n",
      "1   2024-07-22 11:00:00\n",
      "2   2024-07-22 11:00:00\n",
      "3   2024-07-22 12:00:00\n",
      "4   2024-07-22 12:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722131605.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722131605.csv.zip ...\n",
      "File found for 2024-07-22 13:16:05\n",
      "Downloading HABP_1H_SYNOP_20240722131605.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722131605.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 13:00:00\n",
      "1   2024-07-22 13:00:00\n",
      "2   2024-07-22 13:00:00\n",
      "3   2024-07-22 13:00:00\n",
      "4   2024-07-22 13:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722131605.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722131605.csv.zip ...\n",
      "File found for 2024-07-22 13:16:05\n",
      "Downloading HABP_1H_SYNOP_20240722131605.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722131605.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 13:00:00\n",
      "1   2024-07-22 13:00:00\n",
      "2   2024-07-22 13:00:00\n",
      "3   2024-07-22 13:00:00\n",
      "4   2024-07-22 13:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722141605.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722141605.csv.zip ...\n",
      "File found for 2024-07-22 14:16:05\n",
      "Downloading HABP_1H_SYNOP_20240722141605.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722141605.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 00:00:00\n",
      "1   2024-07-22 00:00:00\n",
      "2   2024-07-22 01:00:00\n",
      "3   2024-07-22 02:00:00\n",
      "4   2024-07-22 03:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722141605.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722141605.csv.zip ...\n",
      "File found for 2024-07-22 14:16:05\n",
      "Downloading HABP_1H_SYNOP_20240722141605.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722141605.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 00:00:00\n",
      "1   2024-07-22 00:00:00\n",
      "2   2024-07-22 01:00:00\n",
      "3   2024-07-22 02:00:00\n",
      "4   2024-07-22 03:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722151605.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722151605.csv.zip ...\n",
      "File found for 2024-07-22 15:16:05\n",
      "Downloading HABP_1H_SYNOP_20240722151605.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722151605.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 13:00:00\n",
      "1   2024-07-22 14:00:00\n",
      "2   2024-07-22 15:00:00\n",
      "3   2024-07-22 15:00:00\n",
      "4   2024-07-22 15:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722151605.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722151605.csv.zip ...\n",
      "File found for 2024-07-22 15:16:05\n",
      "Downloading HABP_1H_SYNOP_20240722151605.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722151605.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 13:00:00\n",
      "1   2024-07-22 14:00:00\n",
      "2   2024-07-22 15:00:00\n",
      "3   2024-07-22 15:00:00\n",
      "4   2024-07-22 15:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722161605.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722161605.csv.zip ...\n",
      "File found for 2024-07-22 16:16:05\n",
      "Downloading HABP_1H_SYNOP_20240722161605.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722161605.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 15:00:00\n",
      "1   2024-07-22 16:00:00\n",
      "2   2024-07-22 16:00:00\n",
      "3   2024-07-22 16:00:00\n",
      "4   2024-07-22 16:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722161605.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722161605.csv.zip ...\n",
      "File found for 2024-07-22 16:16:05\n",
      "Downloading HABP_1H_SYNOP_20240722161605.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722161605.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 15:00:00\n",
      "1   2024-07-22 16:00:00\n",
      "2   2024-07-22 16:00:00\n",
      "3   2024-07-22 16:00:00\n",
      "4   2024-07-22 16:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722171603.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722171603.csv.zip ...\n",
      "File found for 2024-07-22 17:16:03\n",
      "Downloading HABP_1H_SYNOP_20240722171603.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722171603.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 17:00:00\n",
      "1   2024-07-22 17:00:00\n",
      "2   2024-07-22 17:00:00\n",
      "3   2024-07-22 17:00:00\n",
      "4   2024-07-22 17:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722171603.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722171603.csv.zip ...\n",
      "File found for 2024-07-22 17:16:03\n",
      "Downloading HABP_1H_SYNOP_20240722171603.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722171603.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 17:00:00\n",
      "1   2024-07-22 17:00:00\n",
      "2   2024-07-22 17:00:00\n",
      "3   2024-07-22 17:00:00\n",
      "4   2024-07-22 17:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722181605.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722181605.csv.zip ...\n",
      "File found for 2024-07-22 18:16:05\n",
      "Downloading HABP_1H_SYNOP_20240722181605.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722181605.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 00:00:00\n",
      "1   2024-07-22 07:00:00\n",
      "2   2024-07-22 07:00:00\n",
      "3   2024-07-22 08:00:00\n",
      "4   2024-07-22 08:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722181605.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722181605.csv.zip ...\n",
      "File found for 2024-07-22 18:16:05\n",
      "Downloading HABP_1H_SYNOP_20240722181605.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722181605.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 00:00:00\n",
      "1   2024-07-22 07:00:00\n",
      "2   2024-07-22 07:00:00\n",
      "3   2024-07-22 08:00:00\n",
      "4   2024-07-22 08:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722191604.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722191604.csv.zip ...\n",
      "File found for 2024-07-22 19:16:04\n",
      "Downloading HABP_1H_SYNOP_20240722191604.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722191604.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 18:00:00\n",
      "1   2024-07-22 19:00:00\n",
      "2   2024-07-22 19:00:00\n",
      "3   2024-07-22 19:00:00\n",
      "4   2024-07-22 19:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722191604.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722191604.csv.zip ...\n",
      "File found for 2024-07-22 19:16:04\n",
      "Downloading HABP_1H_SYNOP_20240722191604.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722191604.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 18:00:00\n",
      "1   2024-07-22 19:00:00\n",
      "2   2024-07-22 19:00:00\n",
      "3   2024-07-22 19:00:00\n",
      "4   2024-07-22 19:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722201603.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722201603.csv.zip ...\n",
      "File found for 2024-07-22 20:16:03\n",
      "Downloading HABP_1H_SYNOP_20240722201603.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722201603.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 19:00:00\n",
      "1   2024-07-22 20:00:00\n",
      "2   2024-07-22 20:00:00\n",
      "3   2024-07-22 20:00:00\n",
      "4   2024-07-22 20:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722201603.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722201603.csv.zip ...\n",
      "File found for 2024-07-22 20:16:03\n",
      "Downloading HABP_1H_SYNOP_20240722201603.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722201603.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 19:00:00\n",
      "1   2024-07-22 20:00:00\n",
      "2   2024-07-22 20:00:00\n",
      "3   2024-07-22 20:00:00\n",
      "4   2024-07-22 20:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722211603.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722211603.csv.zip ...\n",
      "File found for 2024-07-22 21:16:03\n",
      "Downloading HABP_1H_SYNOP_20240722211603.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722211603.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 20:00:00\n",
      "1   2024-07-22 21:00:00\n",
      "2   2024-07-22 21:00:00\n",
      "3   2024-07-22 21:00:00\n",
      "4   2024-07-22 21:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722211603.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722211603.csv.zip ...\n",
      "File found for 2024-07-22 21:16:03\n",
      "Downloading HABP_1H_SYNOP_20240722211603.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722211603.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 20:00:00\n",
      "1   2024-07-22 21:00:00\n",
      "2   2024-07-22 21:00:00\n",
      "3   2024-07-22 21:00:00\n",
      "4   2024-07-22 21:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722221604.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722221604.csv.zip ...\n",
      "File found for 2024-07-22 22:16:04\n",
      "Downloading HABP_1H_SYNOP_20240722221604.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722221604.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 22:00:00\n",
      "1   2024-07-22 22:00:00\n",
      "2   2024-07-22 22:00:00\n",
      "3   2024-07-22 22:00:00\n",
      "4   2024-07-22 22:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722221604.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722221604.csv.zip ...\n",
      "File found for 2024-07-22 22:16:04\n",
      "Downloading HABP_1H_SYNOP_20240722221604.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722221604.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 22:00:00\n",
      "1   2024-07-22 22:00:00\n",
      "2   2024-07-22 22:00:00\n",
      "3   2024-07-22 22:00:00\n",
      "4   2024-07-22 22:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722231606.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722231606.csv.zip ...\n",
      "File found for 2024-07-22 23:16:06\n",
      "Downloading HABP_1H_SYNOP_20240722231606.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722231606.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 22:00:00\n",
      "1   2024-07-22 23:00:00\n",
      "2   2024-07-22 23:00:00\n",
      "3   2024-07-22 23:00:00\n",
      "4   2024-07-22 23:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240722231606.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240722231606.csv.zip ...\n",
      "File found for 2024-07-22 23:16:06\n",
      "Downloading HABP_1H_SYNOP_20240722231606.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240722231606.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 22:00:00\n",
      "1   2024-07-22 23:00:00\n",
      "2   2024-07-22 23:00:00\n",
      "3   2024-07-22 23:00:00\n",
      "4   2024-07-22 23:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240723001603.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240723001603.csv.zip ...\n",
      "File found for 2024-07-23 00:16:03\n",
      "Downloading HABP_1H_SYNOP_20240723001603.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240723001603.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-23\n",
      "1   2024-07-23\n",
      "2   2024-07-23\n",
      "3   2024-07-23\n",
      "4   2024-07-23\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240723001603.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240723001603.csv.zip ...\n",
      "File found for 2024-07-23 00:16:03\n",
      "Downloading HABP_1H_SYNOP_20240723001603.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240723001603.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-23\n",
      "1   2024-07-23\n",
      "2   2024-07-23\n",
      "3   2024-07-23\n",
      "4   2024-07-23\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240723011604.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240723011604.csv.zip ...\n",
      "File found for 2024-07-23 01:16:04\n",
      "Downloading HABP_1H_SYNOP_20240723011604.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240723011604.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-23 00:00:00\n",
      "1   2024-07-23 01:00:00\n",
      "2   2024-07-23 01:00:00\n",
      "3   2024-07-23 01:00:00\n",
      "4   2024-07-23 01:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240723011604.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240723011604.csv.zip ...\n",
      "File found for 2024-07-23 01:16:04\n",
      "Downloading HABP_1H_SYNOP_20240723011604.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240723011604.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-23 00:00:00\n",
      "1   2024-07-23 01:00:00\n",
      "2   2024-07-23 01:00:00\n",
      "3   2024-07-23 01:00:00\n",
      "4   2024-07-23 01:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240723021605.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240723021605.csv.zip ...\n",
      "File found for 2024-07-23 02:16:05\n",
      "Downloading HABP_1H_SYNOP_20240723021605.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240723021605.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-23 01:00:00\n",
      "1   2024-07-23 02:00:00\n",
      "2   2024-07-23 02:00:00\n",
      "3   2024-07-23 02:00:00\n",
      "4   2024-07-23 02:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240723021605.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240723021605.csv.zip ...\n",
      "File found for 2024-07-23 02:16:05\n",
      "Downloading HABP_1H_SYNOP_20240723021605.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240723021605.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-23 01:00:00\n",
      "1   2024-07-23 02:00:00\n",
      "2   2024-07-23 02:00:00\n",
      "3   2024-07-23 02:00:00\n",
      "4   2024-07-23 02:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240723031604.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240723031604.csv.zip ...\n",
      "File found for 2024-07-23 03:16:04\n",
      "Downloading HABP_1H_SYNOP_20240723031604.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240723031604.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-23 02:00:00\n",
      "1   2024-07-23 02:00:00\n",
      "2   2024-07-23 03:00:00\n",
      "3   2024-07-23 03:00:00\n",
      "4   2024-07-23 03:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240723031604.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240723031604.csv.zip ...\n",
      "File found for 2024-07-23 03:16:04\n",
      "Downloading HABP_1H_SYNOP_20240723031604.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240723031604.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-23 02:00:00\n",
      "1   2024-07-23 02:00:00\n",
      "2   2024-07-23 03:00:00\n",
      "3   2024-07-23 03:00:00\n",
      "4   2024-07-23 03:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240723041604.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240723041604.csv.zip ...\n",
      "File found for 2024-07-23 04:16:04\n",
      "Downloading HABP_1H_SYNOP_20240723041604.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240723041604.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-23 04:00:00\n",
      "1   2024-07-23 04:00:00\n",
      "2   2024-07-23 04:00:00\n",
      "3   2024-07-23 04:00:00\n",
      "4   2024-07-23 04:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240723041604.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240723041604.csv.zip ...\n",
      "File found for 2024-07-23 04:16:04\n",
      "Downloading HABP_1H_SYNOP_20240723041604.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240723041604.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-23 04:00:00\n",
      "1   2024-07-23 04:00:00\n",
      "2   2024-07-23 04:00:00\n",
      "3   2024-07-23 04:00:00\n",
      "4   2024-07-23 04:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240723051604.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240723051604.csv.zip ...\n",
      "File found for 2024-07-23 05:16:04\n",
      "Downloading HABP_1H_SYNOP_20240723051604.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240723051604.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-23 05:00:00\n",
      "1   2024-07-23 05:00:00\n",
      "2   2024-07-23 05:00:00\n",
      "3   2024-07-23 05:00:00\n",
      "4   2024-07-23 05:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240723051604.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240723051604.csv.zip ...\n",
      "File found for 2024-07-23 05:16:04\n",
      "Downloading HABP_1H_SYNOP_20240723051604.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240723051604.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-23 05:00:00\n",
      "1   2024-07-23 05:00:00\n",
      "2   2024-07-23 05:00:00\n",
      "3   2024-07-23 05:00:00\n",
      "4   2024-07-23 05:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240723061604.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240723061604.csv.zip ...\n",
      "File found for 2024-07-23 06:16:04\n",
      "Downloading HABP_1H_SYNOP_20240723061604.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240723061604.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-23 05:00:00\n",
      "1   2024-07-23 05:00:00\n",
      "2   2024-07-23 05:00:00\n",
      "3   2024-07-23 05:00:00\n",
      "4   2024-07-23 05:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240723061604.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240723061604.csv.zip ...\n",
      "File found for 2024-07-23 06:16:04\n",
      "Downloading HABP_1H_SYNOP_20240723061604.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240723061604.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-23 05:00:00\n",
      "1   2024-07-23 05:00:00\n",
      "2   2024-07-23 05:00:00\n",
      "3   2024-07-23 05:00:00\n",
      "4   2024-07-23 05:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240723071605.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240723071605.csv.zip ...\n",
      "File found for 2024-07-23 07:16:05\n",
      "Downloading HABP_1H_SYNOP_20240723071605.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240723071605.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 18:00:00\n",
      "1   2024-07-22 18:00:00\n",
      "2   2024-07-22 19:00:00\n",
      "3   2024-07-22 19:00:00\n",
      "4   2024-07-22 20:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_20240723071605.csv.zip\n",
      "getting date from HABP_1H_SYNOP_20240723071605.csv.zip ...\n",
      "File found for 2024-07-23 07:16:05\n",
      "Downloading HABP_1H_SYNOP_20240723071605.csv.zip\n",
      "Response Code: 200\n",
      "Extracting ../data/odp_met_hourly/HABP_1H_SYNOP_20240723071605.csv.zip \n",
      "Index(['Time', 'StationNumber', 'StationName                             ',\n",
      "       'Latitude', 'Longitude', 'Elevation', 'r', 'Q_r', 't', 'Q_t', 'ta',\n",
      "       'Q_ta', 'tn', 'Q_tn', 'tx', 'Q_tx', 'v', 'Q_v', 'p', 'Q_p', 'u', 'Q_u',\n",
      "       'sg', 'Q_sg', 'sr', 'Q_sr', 'suv', 'Q_suv', 'fs', 'Q_fs', 'fsd',\n",
      "       'Q_fsd', 'fx', 'Q_fx', 'fxd', 'Q_fxd', 'fxdat', 'Q_fxdat', 'we', 'Q_we',\n",
      "       'p0', 'Q_p0', 'f', 'Q_f', 'fd', 'Q_fd', 'et5', 'Q_et5', 'et10',\n",
      "       'Q_et10', 'et20', 'Q_et20', 'et50', 'Q_et50', 'et100', 'Q_et100', 'tsn',\n",
      "       'Q_tsn', 'tviz', 'Q_tviz', 'EOR'],\n",
      "      dtype='object')\n",
      "0   2024-07-22 18:00:00\n",
      "1   2024-07-22 18:00:00\n",
      "2   2024-07-22 19:00:00\n",
      "3   2024-07-22 19:00:00\n",
      "4   2024-07-22 20:00:00\n",
      "Name: Time, dtype: datetime64[ns]\n",
      "HABP_1H_SYNOP_LATEST.csv.zip\n",
      "getting date from HABP_1H_SYNOP_LATEST.csv.zip ...\n",
      "Exception: time data \"LATEST\" doesn't match format \"%Y%m%d%H%M%S\", at position 0. You might want to try:\n",
      "    - passing `format` if your strings have a consistent format;\n",
      "    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n",
      "    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\n",
      "File found for None\n",
      "Not per date measurement file, SKIPPED\n",
      "HABP_1H_SYNOP_LATEST.csv.zip\n",
      "getting date from HABP_1H_SYNOP_LATEST.csv.zip ...\n",
      "Exception: time data \"LATEST\" doesn't match format \"%Y%m%d%H%M%S\", at position 0. You might want to try:\n",
      "    - passing `format` if your strings have a consistent format;\n",
      "    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n",
      "    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\n",
      "File found for None\n",
      "Not per date measurement file, SKIPPED\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "from datetime import date\n",
    "\n",
    "domain = \"https://odp.met.hu/\"\n",
    "# this contains daily measurements for all the the stations - with the station data \n",
    "url=\"https://odp.met.hu/weather/weather_reports/synoptic/hungary/hourly/csv/\"\n",
    "filetype=\".zip\"\n",
    "data_table_name= 'weather'\n",
    "\n",
    "\n",
    "def get_soup(url):\n",
    "    return bs(requests.get(url).text,'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "count = 0\n",
    "dates = get_update_times_already_loaded(engine)\n",
    "new_data= []\n",
    "for link in get_soup(url).find_all('a'):\n",
    "    file_link= link.get('href')\n",
    "    #print(link)\n",
    "    if filetype not in file_link:\n",
    "        print('Not searched file {}'.format(file_link))\n",
    "        continue\n",
    "    print(file_link)\n",
    "    d = get_datetime(file_link)\n",
    "    print('File found for {}'.format(d))\n",
    "    if d in dates:\n",
    "        print('Already loaded {}, SKIPPED'.format(d))\n",
    "        continue\n",
    "    if not d: \n",
    "        print('Not per date measurement file, SKIPPED')\n",
    "        continue\n",
    "    print('Downloading {}'.format(file_link))\n",
    "\n",
    "    #zip_folder=data_folder+file_link.split('.')[0]+\"/\"\n",
    "    with open(data_folder+ file_link,'wb') as file:\n",
    "        response = requests.get(url+file_link)\n",
    "        print('Response Code:', response.status_code)\n",
    "        file.write(response.content)\n",
    "    #print('F',data_folder)\n",
    "    #print('FL',file_link)\n",
    "    extracted_folder=data_folder+'extracted/'+file_link.split('.')[0]+'/'\n",
    "    print(f'Extracting {data_folder+file_link} ')\n",
    "    with zipfile.ZipFile(data_folder+file_link, 'r') as zip_ref:\n",
    "        if not os.path.exists(extracted_folder):\n",
    "            os.mkdir(extracted_folder)\n",
    "        zip_ref.extractall(extracted_folder)\n",
    "        for f in os.listdir(extracted_folder):\n",
    "            df = pd.read_csv(extracted_folder + f, sep=\";\",skipinitialspace=True)\n",
    "            print(df.keys())\n",
    "            df['Time']=  pd.to_datetime(df['Time'], format='%Y%m%d%H%M%S')\n",
    "            print(df['Time'].head())\n",
    "\n",
    "            append_to_table(data_table_name,df,engine)\n",
    "    days_df=pd.DataFrame.from_dict({'Time': [pd.to_datetime(d)]})\n",
    "    append_to_table(day_table_name,days_df,engine)\n",
    "\n",
    "    \n",
    "    \n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9e3433ce7162975",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.250952Z",
     "start_time": "2024-07-23T07:54:45.117452Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>StationNumber</th>\n",
       "      <th>StationName</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>r</th>\n",
       "      <th>Q_r</th>\n",
       "      <th>t</th>\n",
       "      <th>Q_t</th>\n",
       "      <th>...</th>\n",
       "      <th>Q_et20</th>\n",
       "      <th>et50</th>\n",
       "      <th>Q_et50</th>\n",
       "      <th>et100</th>\n",
       "      <th>Q_et100</th>\n",
       "      <th>tsn</th>\n",
       "      <th>Q_tsn</th>\n",
       "      <th>tviz</th>\n",
       "      <th>Q_tviz</th>\n",
       "      <th>EOR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-07-22 18:00:00</td>\n",
       "      <td>38537</td>\n",
       "      <td>Boda</td>\n",
       "      <td>46.0867</td>\n",
       "      <td>18.0464</td>\n",
       "      <td>233.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-07-22 18:00:00</td>\n",
       "      <td>58102</td>\n",
       "      <td>Szeged belterület</td>\n",
       "      <td>46.2472</td>\n",
       "      <td>20.1406</td>\n",
       "      <td>103.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-07-22 19:00:00</td>\n",
       "      <td>38537</td>\n",
       "      <td>Boda</td>\n",
       "      <td>46.0867</td>\n",
       "      <td>18.0464</td>\n",
       "      <td>233.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-07-22 19:00:00</td>\n",
       "      <td>58102</td>\n",
       "      <td>Szeged belterület</td>\n",
       "      <td>46.2472</td>\n",
       "      <td>20.1406</td>\n",
       "      <td>103.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-07-22 20:00:00</td>\n",
       "      <td>38537</td>\n",
       "      <td>Boda</td>\n",
       "      <td>46.0867</td>\n",
       "      <td>18.0464</td>\n",
       "      <td>233.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EOR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Time  StationNumber  \\\n",
       "0 2024-07-22 18:00:00          38537   \n",
       "1 2024-07-22 18:00:00          58102   \n",
       "2 2024-07-22 19:00:00          38537   \n",
       "3 2024-07-22 19:00:00          58102   \n",
       "4 2024-07-22 20:00:00          38537   \n",
       "\n",
       "   StationName                               Latitude  Longitude  Elevation  \\\n",
       "0  Boda                                       46.0867    18.0464      233.0   \n",
       "1  Szeged belterület                          46.2472    20.1406      103.9   \n",
       "2  Boda                                       46.0867    18.0464      233.0   \n",
       "3  Szeged belterület                          46.2472    20.1406      103.9   \n",
       "4  Boda                                       46.0867    18.0464      233.0   \n",
       "\n",
       "     r  Q_r      t  Q_t  ...  Q_et20   et50  Q_et50  et100  Q_et100    tsn  \\\n",
       "0  0.0  NaN -999.0  NaN  ...     NaN -999.0     NaN -999.0      NaN -999.0   \n",
       "1  0.0  NaN -999.0  NaN  ...     NaN -999.0     NaN -999.0      NaN -999.0   \n",
       "2  0.0  NaN -999.0  NaN  ...     NaN -999.0     NaN -999.0      NaN -999.0   \n",
       "3  0.0  NaN -999.0  NaN  ...     NaN -999.0     NaN -999.0      NaN -999.0   \n",
       "4  0.0  NaN -999.0  NaN  ...     NaN -999.0     NaN -999.0      NaN -999.0   \n",
       "\n",
       "   Q_tsn   tviz  Q_tviz  EOR  \n",
       "0    NaN -999.0     NaN  EOR  \n",
       "1    NaN -999.0     NaN  EOR  \n",
       "2    NaN -999.0     NaN  EOR  \n",
       "3    NaN -999.0     NaN  EOR  \n",
       "4    NaN -999.0     NaN  EOR  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "850bffc097fec570",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.251264Z",
     "start_time": "2024-07-23T07:54:45.138985Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-23 09:57:21 2024-07-16 09:57:21\n",
      "SELECT \"Time\", u, ta, \"Latitude\", \"Longitude\", \"StationNumber\" FROM weather WHERE \"Time\" >= '2024-07-16 09:57:21' AND \"Time\" < '2024-07-23 09:57:21'\n",
      "                    Time    u     ta  Latitude  Longitude  StationNumber\n",
      "0    2024-07-21 19:00:00   76   24.2   46.4400    17.3744          27615\n",
      "1    2024-07-22 06:00:00 -999 -999.0   46.0867    18.0464          38537\n",
      "2    2024-07-22 06:00:00   80   23.0   46.9778    21.0544          66103\n",
      "3    2024-07-22 07:00:00   61   25.4   46.6172    17.8703          27406\n",
      "4    2024-07-22 07:00:00   67   25.6   47.5200    20.1489          54107\n",
      "...                  ...  ...    ...       ...        ...            ...\n",
      "6777 2024-07-23 07:00:00   66   24.7   48.1042    22.7694          72805\n",
      "6778 2024-07-23 07:00:00   72   23.6   47.8636    22.2222          73110\n",
      "6779 2024-07-23 07:00:00   71   23.7   47.8456    22.6631          73313\n",
      "6780 2024-07-23 07:00:00   63   25.2   47.6961    22.0569          73505\n",
      "6781 2024-07-23 07:00:00   64   25.3   47.4400    22.0011          74500\n",
      "\n",
      "[6782 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# read table data using sql query \n",
    "sd=pd.Timestamp.now()\n",
    "ed=pd.Timestamp.now()-pd.to_timedelta(7, unit='d')\n",
    "\n",
    "sd=sd.strftime('%Y-%m-%d %H:%M:%S')\n",
    "ed=ed.strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(sd,ed)\n",
    "# ,\\\"StationNumber\\\",\\\"StationName\\\"\n",
    "q= \"SELECT \\\"Time\\\", u, ta, \\\"Latitude\\\", \\\"Longitude\\\", \\\"StationNumber\\\" FROM weather WHERE \\\"Time\\\" >= '{}' AND \\\"Time\\\" < '{}'\".format(ed,sd)\n",
    "print(q)\n",
    "\n",
    "sql_df = pd.read_sql( \n",
    "    q, \n",
    "    con=engine  \n",
    ") \n",
    "# print the postgresql table loaded as  \n",
    "# pandas dataframe \n",
    "print(sql_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "688dc4cca1d32dc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.251393Z",
     "start_time": "2024-07-23T07:54:45.184017Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql_df['TRH9010_cond']= (sql_df['ta'] >15) & (sql_df['ta']< 30) & (sql_df['u']>90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e4f4ee6b04f3a",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91bb8b1249e62391",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "260836b0f928c473",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.251508Z",
     "start_time": "2024-07-23T07:54:45.190858Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52a635f513722846",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.251619Z",
     "start_time": "2024-07-23T07:54:45.194771Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bb843228473f4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.251730Z",
     "start_time": "2024-07-23T07:54:45.197905Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 =pd.DataFrame({ \"cond\" :pd.Series([True,True,False,False,False,True])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47fd573e9a8b39ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.252322Z",
     "start_time": "2024-07-23T07:54:45.205643Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cond</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cond\n",
       "0   True\n",
       "1   True\n",
       "2  False\n",
       "3  False\n",
       "4  False\n",
       "5   True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d06e68105ab4c96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.252445Z",
     "start_time": "2024-07-23T07:54:45.210682Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df1 = pd.DataFrame(\\\n",
    "    {\n",
    "        'StationNumber':[1,1,1,1,2,2,2,2,2,1,1,1,3,3,3,3,3,3,3,3],\n",
    "        'TRH9010_cond':[False,False,False,True,False,False,False,False,False,True,True,False,False,True,True,False,True,True,True, True] }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ee0c0cf6025c8dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.252926Z",
     "start_time": "2024-07-23T07:54:45.218016Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StationNumber</th>\n",
       "      <th>TRH9010_cond</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    StationNumber  TRH9010_cond\n",
       "0               1         False\n",
       "1               1         False\n",
       "2               1         False\n",
       "3               1          True\n",
       "4               2         False\n",
       "5               2         False\n",
       "6               2         False\n",
       "7               2         False\n",
       "8               2         False\n",
       "9               1          True\n",
       "10              1          True\n",
       "11              1         False\n",
       "12              3         False\n",
       "13              3          True\n",
       "14              3          True\n",
       "15              3         False\n",
       "16              3          True\n",
       "17              3          True\n",
       "18              3          True\n",
       "19              3          True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99f3029d000874d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.253053Z",
     "start_time": "2024-07-23T07:54:45.221496Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function for calculating p -value\n",
    "# TODO do we need sorting by time? \n",
    "def caculate_p(df1):\n",
    "    df=df1.copy()\n",
    "    df['equal_last'] = df.groupby('StationNumber').apply(\n",
    "        lambda x: x['TRH9010_cond'] == x['TRH9010_cond'].shift(1)).to_list()\n",
    "    # did the truth value of the condition change? we set 1, if there was no change\n",
    "    df['crossing'] = df['equal_last'].map({True: np.nan, False: 1})\n",
    "    # number of  hours without  change\n",
    "    a = df.groupby('StationNumber').apply(lambda x: x['crossing'].cumsum().ffill())\n",
    "    #  stage = number of the current period without change, for grouping and counting the length of it \n",
    "    df['stage'] = a.reset_index(level=0, drop=True)\n",
    "    # for how many hours there was no change\n",
    "    # here should add history, stage should be set to 0 if no change between it\n",
    "    \n",
    "    df['TRH9010'] = df.groupby(['StationNumber', 'stage'])['TRH9010_cond'].transform('cumcount')\n",
    "    # fusarium p \n",
    "    df['p']=0.0\n",
    "    df.loc[(df['TRH9010_cond']==True) & (df['TRH9010']>0),'p']=6.8128*df['TRH9010']-3.3756\n",
    "    return df.drop(columns=['equal_last','crossing','stage'])\n",
    "#caculate_p(df)\n",
    "def add_history(df_hist,df1):\n",
    "    df2 = df1.reset_index()\n",
    "    firsts = df2.groupby('StationNumber').first()\n",
    "    firsts.set_index('index')\n",
    "    lasts = df_hist.groupby('StationNumber').last().reset_index()\n",
    "    merged = firsts.merge(lasts,on='StationNumber',suffixes=('','_last'))\n",
    "\n",
    "    merged['TRH9010'] = np.where(merged['TRH9010_cond'] == merged['TRH9010_cond_last'],\n",
    "                                 1 + merged['TRH9010_last'], merged['TRH9010'])\n",
    "    new_counts = merged.loc[:, ~merged.columns.str.endswith('_last')]\n",
    "    df1.loc[firsts.index] = merged\n",
    "    return df1\n",
    "\n",
    "def caculate_p_incremental(df1, history_df):\n",
    "    df=df1.copy()\n",
    "    df['equal_last'] = df.groupby('StationNumber').apply(\n",
    "        lambda x: x['TRH9010_cond'] == x['TRH9010_cond'].shift(1)).to_list()\n",
    "    # did the truth value of the condition change? we set 1, if there was no change\n",
    "    df['crossing'] = df['equal_last'].map({True: np.nan, False: 1})\n",
    "    # number of  hours without  change\n",
    "    a = df.groupby('StationNumber').apply(lambda x: x['crossing'].cumsum().ffill())\n",
    "    #  stage = number of the current period without change, for grouping and counting the length of it \n",
    "    df['stage'] = a.reset_index(level=0, drop=True)\n",
    "    # for how many hours there was no change\n",
    "    # here should add history, stage should be set to 0 if no change between it\n",
    "    df['TRH9010'] = df.groupby(['StationNumber', 'stage'])['TRH9010_cond'].transform('cumcount')\n",
    "    df = add_history(history_df,df)\n",
    "    # fusarium p \n",
    "    df['p']=0.0\n",
    "    df.loc[(df['TRH9010_cond']==True) & (df['TRH9010']>0),'p']=6.8128*df['TRH9010']-3.3756\n",
    "    return df.drop(columns=['equal_last','crossing','stage'])\n",
    "\n",
    "\n",
    "def find_stages(df1):\n",
    "    df=df1.copy()\n",
    "    df['equal_last'] = df.groupby('StationNumber').apply(\n",
    "        lambda x: x['TRH9010_cond'] == x['TRH9010_cond'].shift(1)).to_list()\n",
    "    # did the truth value of the condition change? we set 1, if there was no change\n",
    "    df['crossing'] = df['equal_last'].map({True: np.nan, False: 1})\n",
    "    # number of  hours without  change\n",
    "    a = df.groupby('StationNumber').apply(lambda x: x['crossing'].cumsum().ffill())\n",
    "    #  stage = number of the current period without change, for grouping and counting the length of it \n",
    "    df['stage'] = a.reset_index(level=0, drop=True)\n",
    "    # for how many hours there was no change\n",
    "    # here should add history, stage should be set to 0 if no change between it\n",
    "    df['TRH9010'] = df.groupby(['StationNumber', 'stage'])['TRH9010_cond'].transform('cumcount')\n",
    "    return df\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "787d5df1b69f2618",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.253774Z",
     "start_time": "2024-07-23T07:54:45.241655Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/j3p6g91d6dj8wzlq0vy4dw8r0000gn/T/ipykernel_52273/998563707.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['equal_last'] = df.groupby('StationNumber').apply(\n",
      "/var/folders/sp/j3p6g91d6dj8wzlq0vy4dw8r0000gn/T/ipykernel_52273/998563707.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  a = df.groupby('StationNumber').apply(lambda x: x['crossing'].cumsum().ffill())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StationNumber</th>\n",
       "      <th>TRH9010_cond</th>\n",
       "      <th>TRH9010</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.4372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.4372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.4372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    StationNumber  TRH9010_cond  TRH9010        p\n",
       "0               1         False      0.0   0.0000\n",
       "1               1         False      1.0   0.0000\n",
       "2               1         False      2.0   0.0000\n",
       "3               1          True      0.0   0.0000\n",
       "4               2         False      NaN   0.0000\n",
       "5               2         False      NaN   0.0000\n",
       "6               2         False      0.0   0.0000\n",
       "7               2         False      0.0   0.0000\n",
       "8               2         False      1.0   0.0000\n",
       "9               1          True      1.0   3.4372\n",
       "10              1          True      2.0  10.2500\n",
       "11              1         False      3.0   0.0000\n",
       "12              3         False      0.0   0.0000\n",
       "13              3          True      0.0   0.0000\n",
       "14              3          True      1.0   3.4372\n",
       "15              3         False      0.0   0.0000\n",
       "16              3          True      0.0   0.0000\n",
       "17              3          True      1.0   3.4372\n",
       "18              3          True      2.0  10.2500\n",
       "19              3          True      3.0  17.0628"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caculate_p(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78a1372290e2acbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.254269Z",
     "start_time": "2024-07-23T07:54:45.255488Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StationNumber</th>\n",
       "      <th>TRH9010_cond</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    StationNumber  TRH9010_cond\n",
       "0               1         False\n",
       "1               1         False\n",
       "2               1         False\n",
       "3               1          True\n",
       "4               2         False\n",
       "5               2         False\n",
       "6               2         False\n",
       "7               2         False\n",
       "8               2         False\n",
       "9               1          True\n",
       "10              1          True\n",
       "11              1         False\n",
       "12              3         False\n",
       "13              3          True\n",
       "14              3          True\n",
       "15              3         False\n",
       "16              3          True\n",
       "17              3          True\n",
       "18              3          True\n",
       "19              3          True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "824135bf0f9ec70b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.254394Z",
     "start_time": "2024-07-23T07:54:45.265223Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = df1.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fad54144698f87",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbe5929f57e895f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.254501Z",
     "start_time": "2024-07-23T07:54:45.267402Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "firsts = df2.groupby('StationNumber').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed95d14cc63d188f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.254877Z",
     "start_time": "2024-07-23T07:54:45.276686Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>TRH9010_cond</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StationNumber</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               index  TRH9010_cond\n",
       "StationNumber                     \n",
       "1                  0         False\n",
       "2                  4         False\n",
       "3                 12         False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firsts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4b6ed8215a4b5c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.273264Z",
     "start_time": "2024-07-23T07:54:45.288039Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRH9010_cond</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TRH9010_cond\n",
       "index              \n",
       "0             False\n",
       "4             False\n",
       "12            False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firsts.set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "522b93b5e2931676",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.273538Z",
     "start_time": "2024-07-23T07:54:45.300829Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.loc[firsts.index]=firsts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "441a799179ffd775",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.273942Z",
     "start_time": "2024-07-23T07:54:45.303890Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([1, 2, 3], dtype='int64', name='StationNumber')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firsts.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "edec93cdd1b91834",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.274211Z",
     "start_time": "2024-07-23T07:54:45.312740Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lasts = df2.groupby('StationNumber').last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e93087cdc41d469",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.299939Z",
     "start_time": "2024-07-23T07:54:45.316692Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e4399e367e8e8af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.300130Z",
     "start_time": "2024-07-23T07:54:45.320031Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lasts = df1.groupby('StationNumber').last().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ee91b2e0c40a3b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.300241Z",
     "start_time": "2024-07-23T07:54:45.324721Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged = firsts.merge(lasts,on='StationNumber',suffixes=('','_last'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fdf552cb067ed35e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.614225Z",
     "start_time": "2024-07-23T07:54:45.337657Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StationNumber</th>\n",
       "      <th>index</th>\n",
       "      <th>TRH9010_cond</th>\n",
       "      <th>TRH9010_cond_last</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   StationNumber  index  TRH9010_cond  TRH9010_cond_last\n",
       "0              1      0         False              False\n",
       "1              2      4         False              False\n",
       "2              3     12         False               True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1138b4fcf8f561",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.614433Z",
     "start_time": "2024-07-23T07:54:45.342669Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7acef469de9fc424",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:54:47.779831Z",
     "start_time": "2024-07-23T07:54:45.346166Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'TRH9010'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/fusarium_env1/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mindex.pyx:196\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'TRH9010'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[39], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m merged[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTRH9010\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mwhere(merged[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTRH9010_cond\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m==\u001B[39mmerged[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTRH9010_cond_last\u001B[39m\u001B[38;5;124m'\u001B[39m],\u001B[43mmerged\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mTRH9010\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m+\u001B[39mmerged[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTRH9010_last\u001B[39m\u001B[38;5;124m'\u001B[39m],merged[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTRH9010\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m~/anaconda3/envs/fusarium_env1/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   4100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   4101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 4102\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   4104\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m~/anaconda3/envs/fusarium_env1/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m   3808\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[1;32m   3809\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[1;32m   3810\u001B[0m     ):\n\u001B[1;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[0;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3817\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'TRH9010'"
     ]
    }
   ],
   "source": [
    "merged['TRH9010']=np.where(merged['TRH9010_cond']==merged['TRH9010_cond_last'],merged['TRH9010']+merged['TRH9010_last'],merged['TRH9010'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46de75ddbf1f8a1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:45.946412Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_counts=merged.loc[:,~merged.columns.str.endswith('_last')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0939a2ca1498e0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:45.949391Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a191eef44ae5f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:45.952153Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ca92bb860688c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:45.955795Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_map = caculate_p(sql_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be21ac0345760bf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:45.959986Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ebe4a4f110df64",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:45.963884Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_location='../output/'\n",
    "to_map[['Time','u','ta','Latitude','Longitude','p']].to_csv(output_location+'test_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b555aef5f1844973",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:45.966809Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df1 = pd.DataFrame(\\\n",
    "    {\n",
    "        'StationNumber':[1,1,1,1,2,2,2,2,2,1,1,1,3,3,3,3,3,3,3,3],\n",
    "        'TRH9010_cond':[False,False,False,True,False,False,False,False,False,True,True,True,True,True,True,False,True,True,True, True] }) \n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2507fb40d75fbc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:45.970312Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n=find_stages(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc31d96663c9f7c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:45.973345Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = caculate_p(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438558959656666f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:45.977081Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b269bc8099387f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:45.981280Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7474b3c8dcf3ad6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:45.984710Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_new = caculate_p_incremental(h,n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795627569853919b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:45.988467Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30458fcc3f663d62",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:45.991979Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de22aa2c90f4a5f4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:45.996106Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf8d69684265a3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:45.999440Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "add_history(df1,df1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d4e1e388126a0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:46.003621Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91a286a0c3ea53f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:46.006498Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_map = caculate_p(sql_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce368e3f1b6fccf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:46.010530Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0f62d7e2752e15",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:46.013122Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_location='../output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d9b17a3b74cbd",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:46.015273Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_map[['Time','u','ta','Latitude','Longitude','p']].to_csv(output_location+'test_output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aca26f078395526",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# time execution: we should run query every hour \n",
    "\n",
    "[stackoverflow](https://stackoverflow.com/questions/73358409/how-to-schedule-a-script-to-run-every-day-for-python)\n",
    "\n",
    "[schedule lib](https://schedule.readthedocs.io/en/stable/)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "916dc603f959daed",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T08:36:50.855523Z",
     "start_time": "2024-07-23T08:36:20.744888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec 1\n",
      "Exec 1\n",
      "Exec 1\n",
      "Exec 1\n",
      "Exec 1\n"
     ]
    }
   ],
   "source": [
    "import schedule\n",
    "import time\n",
    "i=0\n",
    "\n",
    "def test_f(x):\n",
    "    print(f\"Exec {x}\")\n",
    "\n",
    "    \n",
    "\n",
    "schedule.every(5).seconds.do(test_f, 1)\n",
    "\n",
    "\n",
    "i=0\n",
    "while i<30:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cd7c93f64ec68e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:46.020470Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "fusarium_table_name = os.getenv('FUSARIUM_TABLE_NAME')\n",
    "\n",
    "    \n",
    "\n",
    "def get_fusarium_history(first_date, tn):\n",
    "    \n",
    "    if not table_exists(engine,tn):\n",
    "        return None \n",
    "    #TODO  magic constant\n",
    "    d = first_date - timedelta(minutes=70)\n",
    "    q= \"SELECT \\\"Time\\\", u, ta, \\\"Latitude\\\", \\\"Longitude\\\", \\\"StationNumber\\\", \\\"TRH9010_cond\\\" ,\\\"p\\\" FROM {} WHERE \\\"Time\\\" >= '{}'\".format(tn,d)\n",
    "    print(q)\n",
    "    sql_df = pd.read_sql( \n",
    "        q, \n",
    "        con=engine  \n",
    "    ) \n",
    "    if sql_df.empty:\n",
    "        return None\n",
    "        \n",
    "    \n",
    "\n",
    "def increment_p( df, first_date, hist_tn):\n",
    "    # function for calculating p -value\n",
    "    df['TRH9010_cond']= (df['ta'] >15) & (df['ta']< 30) & (df['u']>90)\n",
    "    df_hist = get_fusarium_history(first_date,hist_tn)\n",
    "    if df_hist:\n",
    "        df = pd.caculate_p_incremental(df_hist,df)\n",
    "        print('adding history.. ')\n",
    "    else:\n",
    "        print('No history to use')\n",
    "        df = caculate_p(df)\n",
    "    # if there is incrementation, otherwise at first fill, we write everything we got\n",
    "    print(df.head())\n",
    "\n",
    "    if df_hist:\n",
    "        df= df[df['Time']>=first_date]\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7726d245bc26905f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:46.022846Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time = df['Time'].min()\n",
    "incremented = increment_p(df,time, fusarium_table_name)\n",
    "incremented = incremented[['Time','u','ta','Latitude','Longitude','p']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49acd9b0783b739c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:46.024967Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fusarium_table_name = os.getenv('FUSARIUM_TABLE_NAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869f83de5589cd55",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:46.027938Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(fusarium_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46593fc596947572",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:46.029597Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "append_to_table(fusarium_table_name,incremented,engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed7ba5095b18251",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:46.031581Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "incremented.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40cafda45b8dedd",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:46.034096Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "incremented.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0882c03fc61687e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:46.035805Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f64bffafeadff7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:46.037440Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sqlalchemy.dialects.mysql import insert\n",
    "\n",
    "def insert_on_duplicate(table, conn, keys, data_iter):\n",
    "    insert_stmt = insert(table.table).values(list(data_iter))\n",
    "    on_duplicate_key_stmt = insert_stmt.on_duplicate_key_update(insert_stmt.inserted)\n",
    "    conn.execute(on_duplicate_key_stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e587b1ef56f6c0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:46.039455Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_sql(fusarium_table_name+\"_\", engine, if_exists='append', chunksize=4096, method=insert_on_duplicate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4028a4f5481d0bd",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:46.042755Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fusarium_table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf3d676747e9dde",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:46.045218Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "int(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b3b2209fa4cb43",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:46.047084Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b959ac90547dfeb5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:46.050245Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ed611d8fab7ea9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:46.052156Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime, pytz\n",
    "    \n",
    "# t is a date after 2262 that should be coerced into NaT\n",
    "t = datetime.datetime(3000, 1, 1, 0, 0, 0, 0, pytz.UTC)\n",
    "\n",
    "# lists with many such dates\n",
    "l50 = [t] * 50\n",
    "l51 = [t] * 51\n",
    "\n",
    "# pd.to_datetime crashes if the list is longer that 50\n",
    "\n",
    "# this is fine\n",
    "print(pd.to_datetime(l50, utc=True, errors='coerce'))  \n",
    "\n",
    "# this crashes\n",
    "print(pd.to_datetime(l51, utc=True, errors='coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a413aae7ce28a6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T07:54:46.053884Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusarium_env1",
   "language": "python",
   "name": "fusarium_env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
